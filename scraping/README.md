# Scraping for data collection

# Group project
**Proposal:** Run scrapy spiders as a daemon process to scrape 
1) tweets, 2) Reddit and 3) Quora and store in PostgreSQL 
database.

## TODO
- select name
- discuss trigger mechanism
- sketch out database schema
- discuss high level API desire (to pull out of database)
- who/what is the consumer of our API- REST? CSV files?


# Examples
- [Scrapy --> Postgresql Tutorial](http://newcoder.io/scrape/intro/)
- [scrapy docs tutorial](https://doc.scrapy.org/en/latest/intro/tutorial.html)
- [Using Scrapy to build your own Dataset](https://towardsdatascience.com/using-scrapy-to-build-your-own-dataset-64ea2d7d4673)

# Scraping
- [User-Agent Tut](https://www.scrapehero.com/how-to-fake-and-rotate-user-agents-using-python-3/)
- [Prevent Getting Blocked](https://www.scrapehero.com/how-to-prevent-getting-blacklisted-while-scraping/)